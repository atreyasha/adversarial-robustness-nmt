*** To-do's

**** Code and documentation
***** TODO create different scripts to do different things -> translate, basic_evaluate (bleu, chrf), fine_tune (large models on English), model evaluate, visualize, etc. -> rate de-de and en-en with chrf scores for some measure, utilize XLM-R on both paraphrases to have some baseline for validity
***** TODO redo repository with only necessary code-chunks and fill up readme, recreate environment with python3.7 in cluster -> re-run simple tests first
***** add a deployed service on GitHub to build and check sanity
***** add a pylinter for formatting as pre-commit hook -> think of standards to abide by -> auto-PEP8
***** add documentation with typing to utils code later on
***** ultimately release best fine-tuned model for use in other scenarios -> if possible add reproducability concept with setting seeds
***** provide readme to hdf5 files with different index meanings
***** consider building readme and project using python -m framework
***** think of how to handle LASER vs. larger model workflows in one repo
***** log out random seeds for full reproducability
***** add citations in readme as per general standard
***** add relevant gitignores
***** add documentation/acknowledgments to datasets and code, and how to handle submodules
***** re-review dependencies and remove unnecessary ones upon next check

**** Investigate semantics transfer during translation
***** TODO investigate PAWS-X data origins and ensure this is not present in the NMT training data -> PAWS-X English is derived from Wikipedia with swapping/back-translation and verified, PAWS-X dev/test sets for all other languages are human translations while PAWS-X non-English training data is machine translated, WMT19 best model was trained on Newstest data which could not have overlapped with Wikipedia data given that in PAWS-X it was human translated -> proceed with analysis despite this and perhaps do a meta-analysis of more languages to gain some meat
***** do a bleu score comparison of translations and gold sentence paraphrases for surface level analysis -> output data as tsv instead of json -> or perhaps provide option for both -> add this as a pipeline to investigate -> perhaps use some combinations of scores where possible
***** utilize fairseq model(s) for translation from English to German and then use test translations, provide options for various models and run these through fine-tuned paraphrase detection model later on -> run on both gold and translated to check for systematic errors as well
***** use XLM-R model for predictions -> predict it on the test set in german, and the test set translated and check the differences and where they coincide and where not -> will tell you where the model is bad and where not to trust it -> and where to trust it
***** check if human evaluation would be necessary at any point
***** expect very good results on translation and think of how to analyze and interpret/explain them

**** Paraphrase data selection and analysis workflow
***** Fine-tuning large X-models
****** TODO fine-tune models with English and ensure no or little machine translated data is present in training set
****** refactor and improve xtreme code with simpler repository -> modify logging of models, combination of languages, correct naming of training parameters and files, make training process exhaustive and thorough, add better metrics for monitoring performance including ROC-AUC etc.
****** train more model combinations, change evaluation metrics on test set to only be at the end, continue training for existing models or re-evaluate them on the test dataset, make new model which learns from all data instead of just one language, remove constant re-writing of caches, add more information into each log file to make it unique
****** change pre-processing from two concatenated sentences to permutation invariant type -> check if it improves
****** use both s3it and local cluster for simultaneous model training
****** data augmentation with easy examples -> perhaps add this in to training scheme
****** look into Elektra, SentenceBERT, bert_score, models developed for GLUE tasks such as paraphrase detection tasks 
****** not good enough argument to show that siamese network is necessary compared to ordered concatenation

***** TODO compare performance with or without other languages to see if this differs -> might be an interesting comparison if results are "boring" for German
***** compare performance with other pre-trained paraphrase detector -> such as fine-tuned multilingual BERT from PAWS-X paper
***** add failsafe to output maximum score in case same inputs
***** better to work with human-curated data than back-translated ones due to many errors -> advantage in PAWS and PAWS-X
***** possible to get students to do tests for us to check for semantic transfer
***** keep documentation of work -> such as SGCP & SOW-REAP performance (with examples), LASER performance

*** Completed
***** DONE bug in XLM-R as it does not appear to learn -> look through code
      CLOSED: [2020-06-17 Wed 16:47]
***** DONE multilingual BERT with de only -> bug in how test scripts are saved leads to wrong results
      CLOSED: [2020-06-17 Wed 16:48]
***** DONE maybe consider using German BERT for doing this task explicitly for German, for our end task -> German BERT and RoBERTa for English to focus on exact task -> perhaps just use xtreme repo and keep only paws-x task -> clean up code and workflow for it -> error might be arising due to gradient clipping for very large model
      CLOSED: [2020-06-17 Wed 16:48]
***** DONE look into ParaBank2 and universal decompositional semantics -> not great paraphrases, no human curation
      CLOSED: [2020-06-05 Fri 14:28]
***** DONE look into Duolingo dataset for paraphrases -> no German target side
      CLOSED: [2020-06-05 Fri 13:56]
***** DONE add symbols for defaults in metavar default formatter, maybe add some other formatting tricks such as indents for defaults
      CLOSED: [2020-06-02 Tue 17:55]
***** DONE try installing java locally instead of root, if stanford parser is indeed necessary
      CLOSED: [2020-05-29 Fri 15:23]
***** DONE paraphrasing with SGCP -> very bad results on both original test and WMT data -> very sensitive to exemplar
      CLOSED: [2020-05-28 Thu 18:14]
***** DONE embed and cluser using universal sentence encoder (eg. BERT or LASER) -> use separate clusters for exemplar utility, make diverse collection and evaluate using metric or other NN
      CLOSED: [2020-05-28 Thu 17:52]
***** DONE find other sentence with maximum similarity and use that as exemplar, useparaphrase of best as exemplar, use pos-tags of sentence
      CLOSED: [2020-05-28 Thu 17:52]
***** DONE convert wmt datasets with derived exemplars into format pipe-able into SGCP -> needed before paraphrasing
      CLOSED: [2020-05-28 Thu 17:52]
***** DONE add workflow to download laser models with python -m laserembeddings download-models
      CLOSED: [2020-05-28 Thu 17:49]
***** DONE set up WMT 17 dev/test data and basic repo
      CLOSED: [2020-04-29 Wed 15:57]
***** DONE convert all processes to makefile for ease
      CLOSED: [2020-05-04 Mon 15:31]
***** DONE set up data downloading for all wmt sets with SacreBLEU
      CLOSED: [2020-05-17 Sun 21:58]

*** Downstream work
**** LASER embeddings + dense layers
***** TODO develop small but efficient pipeline to run LASER + dense layer to get basic performance and show ineffectiveness
***** TODO add function for normalization within class itself -> or think of how to make normalization scheme portable and not have it separate outside of model
***** figure out nicer and more automated means of logging experiments -> tensorboard + csv logging -> consider using wandb, mlflow or comet-ml
***** extend to all combinations of languages, keep this as baseline comparison with larger models

**** Semantic similarity metrics
***** make table with all metrics and various datasets
***** possibly use several language pairs to test this
***** multireference BLEU score, use multiple paraphrases and check for best BLEU score
***** perhaps modified BLEU, METEOR, CCG semantics lambda calculus
***** perhaps some combination of edit distance with wordnet metrics
***** or NN technique using sentence BERT and other encoders -> more quantitative and continuous, can apply Michel et al. 2019 techniques for robustness comparisons
***** semantic parsing to graph, role labelling, wordnet concepts connecting, framenet, frame semantic parsing, brown clusters, AMR parsing, IWCS workshop for discussions 

**** Paraphrase generation
***** Ideas for self-paraphrasing
****** consider logical model for paraphrases, active to passive syntaxes and other logical frameworks -> use dependency parse on manual examples and check for logical process to create meaningful permutations
****** permute-paraphrase using syntax-tree chunks and test paraphrses using a detect or LASER embeddings for agnosticism between source/target

***** Viable pre-developed dynamic paraphrase-generation frameworks
****** SOW-REAP [torch, python3, average-documented] -> generate paraphrases without exemplar sentence form, worth trying out
******* refactor/extract out SOW model, shorten pipeline in sow to reduce computation and make input simpler
******* make quick samples from SOW and hand-select good ones, test them manually on fairseq NMT system for en-de to probe robustness
******* fork sow repo and clean code, remove bugs and make better documented with dep tracking and clearer instructions
******* require nltk word tokenize before main processing

****** SGCP [torch, python3, well-documented] -> generate paraphrases given exemplar sentence form, limitation is that exemplar sentence is a hard dependency, poor performance and not very semantically sound paraphrases
******* ParaNMT is likely to be better than QQPos since latter was trained only on qns
******* BERT score, BERT, RoBERTa for detecting paraphrases and quality
******* hand-written exemplar for meaningful output
******* remove exemplar sentence and replace with syntax form
******* clustering is done by meaning and not syntax -> or try difference via standard parse -> or random
******* provision of syntax directly instead of exemplar sentence
******* fix bug in sgcp to write all outs on separate lines and to not compute any similarity
******* change k means to find best number of clusters
******* add various paraphrase generation styles for SGCP such as same cluster, other cluster and same as source
******* require nltk word tokenize before main processing
******* future-idea: end-to-end paraphrase generation with adversarial goal, but unrealistic given time-frame and support

***** Legacy frameworks
****** Pair-it [tensorflow, python3, poorly documented] -> has potential to work but requires major refactoring
****** SCPN [torch, python2.7, poorly documented] -> buggy, but some examples work

**** Data augmenttion
***** look into nli adversarial datasets -> Nevin and Aatlantise
***** either look for paraphrase source and target pair which are closest to gold ones and augment data with these -> is safer to train with and can possibly improve overall translation quality
***** otherwise, find paraphrase which is close on source side but problematic on target side and augment these with gold target -> acts as a regularizing anchor and possibly adds some stability
***** Zipf's law should apply to syntax chunks, bias might still be present
***** anchor might still be useful, look for similar syntax on the target side that can be substituted -> maybe some kind of imitation to make augmented pairs 
***** consider contributing paraphrases to data augmentation libraries from research
***** noise is not problematic since there is already noise present in normal training data
***** meaning preserving + adversarial outcome -> then useful
***** augmentation is important if adversarial attack is successful, maybe syntax real-life frequency has effect
