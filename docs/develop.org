*** To-do's

**** Code and documentation
***** TODO set up data downloading for all wmt sets with SacreBLEU
***** TODO handle virtual environment in remote system better -> maybe with poetry or with venvs -> either way keep it clean and simple
***** add relevant gitignores
***** consider building readme and project using python -m framework
***** add documentation/acknowledgments to datasets and code, refactor major code used in SCPN to make it cleaner and better
***** add citations in readme as per general standard

**** Paraphrase generation
***** embed and cluser using universal sentence encoder -> use separate clusters for exemplar utility, make diverse collection and evaluate using metric or other NN
***** maximize similarity metric on both sides, use paraphrase of maximum as exemplar, use pos-tags of sentence
***** QQPos is likely to be a better model, check quality of paraphrases, BERT score for quality of paraphrases
***** BERT, RoBERTa for detecting paraphrases

***** Viable frameworks
****** SGCP [torch, python3, well-documented] -> generate paraphrases given exemplar sentence form, limitation is that exemplar sentence is a hard dependency
******* viable-idea: remove exemplar sentence and replace with syntax form
******* future-idea: end-to-end paraphrase generation with adversarial goal, but unrealistic given time-frame and support

***** Legacy frameworks
****** SOW-REAP [torch, python3, average-documented] -> generate paraphrases without exemplar sentence form, worth trying out
****** SCPN [torch, python2.7, poorly documented] -> buggy, but some examples work
****** Pair-it [tensorflow, python3, poorly documented] -> has potential to work but requires major refactoring

**** SOTA NMT models
***** download SOTA models from fairseq, start testing paraphrased samples on it and manually check out differences in results, see if this idea makes sense on a large scale
***** look for models that worked on WMT en-de datasets and work from there

**** Semantic similarity metrics
***** multireference BLEU score, use multiple paraphrases and check for best BLEU score
***** make table with all metrics, or use several language pairs to test this, pre-process data as per pre-trained model
***** think of useful semantic similarity metrics to make comparisons
***** perhaps modified BLEU, METEOR, CCG semantics lambda calculus
***** or NN technique using sentence BERT and other encoders -> more quantitative and continuous, can apply Michel et al. 2019 techniques for robustness comparisons

**** Downstream data augmentation
***** dual approach -> either look for paraphrase source and target pair which are closest to gold ones and augment data with these -> is safer to train with and can possibly improve overall translation quality
***** otherwise, find paraphrase which is close on source side but problematic on target side and augment these with gold target -> acts as a regularizing anchor and possibly adds some stability -> need to check semantics
***** this would be future work, but presentation of work depends on how this is envisioned

*** Completed
***** DONE set up WMT 17 dev/test data and basic repo
      CLOSED: [2020-04-29 Wed 15:57]
***** DONE convert all processes to makefile for ease
      CLOSED: [2020-05-04 Mon 15:31]
***** DONE add pipeline to download WMT 17 training data      
      CLOSED: [2020-05-04 Mon 15:37]
      
