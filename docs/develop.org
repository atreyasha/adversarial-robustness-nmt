*** Development

**** Paper
***** TODO work on writing paper in succinct manner
***** TODO include semantic transferance equation in paper to introduce some formalisms -> show mathematical properties of isometric functions/spaces and how this should hold for semantic vector spaces
***** make less confident conclusion on relationship between back-translation and translation consistency -> could also be linked to other differences between models
***** explan that papers like volatility one might be making claims based on weaker models that could be fixed by using larger models
***** think more about whether to include or exclude adversarial term since this might be a grey area -> qualify various means of being adversarial ie. targetted through model or perhaps just an intention
***** report evaluation of fine-tuning paraphrase detector and weaker translation model
***** describe processes that worked and did not work -> talk about all the hurdles and show some bad examples when they occurred -> summarized below in logs
***** list hypotheses and how some were refuted by results
***** paraphrase detection as a form of isometry quantification as well as evaluation metric for NMT systems
***** add pawsx training on de+en as future task to see how much this would change things

**** Evaluation
***** TODO main source of errors seems to be wrong language insertion in scaling NMT model while not really the case in FAIR SOTA model -> check test data performance to see if this is also the case -> perhaps this is a systematic error for non-backtranslated model
***** TODO look into interesting cases in regards to paraphrase output results, such as (0,1) and (1,0) etc. -> check what these outputs represent in terms of actual sentences and what observations can be drawn from these -> for eg. (0,1) behaviour of FAIR model could point to some regularizing capabilities acquired from good model training and/or backtranslation -> exclude cases of mixed languages
***** check for possibly interesting correlations between XLM-R prediction and chrF/BLEU scores -> this could be of interest in making additional statements to Michel et al. 2019's statements regarding chrF scores
***** in rare cases, can do manual analysis and include this inside report
***** early conclusions/hypotheses: hand-crafted adversarial paraphrase robustness is handled well in SOTA models due to backtranslation reguralization, main vulnerability will be targetted adversarial samples
***** show that training with backtranslation helps for robustness to paraphrases -> through visualizations and perhaps some statistical tests
***** **extra:** compute statistical tests for ascertaining significance of relationships
***** **extra:** consider changing bleu to sacrebleu in json (read more about differences) and figure out why stating this might be important

**** Clean-code and documentation
***** segment readme into training, translation and others categories with relevant usages
***** replace relevant bash commands with sbatch in slurm-s3it branch after repository is completed
***** update initial page of readme with overview/abstract of work including shallow metrics
***** update TOC's in all readmes to reflect latest changes
***** update R dependencies in readme once all visualizations are finalized
***** add version numbers to R package dependencies for posterity
***** add citations in readme as per general standard
***** add final paper/presentation into repo with link in readme 
***** add github repo to paperswithcode examples for relevant papers

**** Paraphrase detection
***** make formal analysis on lengths of WMT19 inputs vs. lengths of paws-x training data
***** consider roc and other evaluation metrics for pawsx model -> in case these might be of more help
***** fine-tune models with English and ensure no or little machine translated data is present in training set
***** better to work with human-curated data than back-translated ones due to many errors -> advantage in PAWS and PAWS-X English data + WMT19 AR paraphrases
***** **extra:** fix and refine paws-x pipeline later on with patience, typing, better documentation, clean-code and possibly continuation of training, add roc auc on pawsx test data

**** Translation
***** strong model being WMT19 single and ensemble with back translation (which adds robustness), while weak model being transformer trained on WMT16 without back translation -> compare general performances and metrics
***** consider also looking into extra references repo "evaluation-of-nmt-bt"
***** possibly keep backups of models at various development stages
***** **extra:** train additional large model on wmt19 non-backtranslated data and similar transformer arch as fair paper -> to get slightly better performance for comparison -> this can also be done later 

**** Visualization
***** **extra:** consider plotting out agreement statistics sampled from uniform distribution within bar chart and if this would be of use
***** **extra:** check if chord or tree mapping plot could be possible to see dependencies and functional mappings
      
*** Completed
***** create modular scripts with instructions in readme: 
****** DONE visualize model training evolutions
****** DONE visualize fine-tuned LM result -> joint view
****** DONE visualize correlation of LM and shallow metrics -> joint view
****** DONE visualize shallow metrics
****** DONE train translation model (after better NMT performance) 
****** DONE translate sentences (after better NMT performance)
****** DONE evaluate using fine-tuned language model
****** DONE fine tune paraphrase detector
****** DONE evaluate bleu & chrf
***** DONE clean up exporting script where user can specify which checkpoint should be packaged
      CLOSED: [2020-07-24 Fri 15:55]
***** DONE replace mean/sd annotations in plots with vector for mean and covariance matrix for sd
      CLOSED: [2020-07-23 Thu 12:00]
***** DONE reduce computational overhead by caching source computations for paraphrase detection evaluation
      CLOSED: [2020-07-22 Wed 12:03]
***** DONE make shell script which automatically filters and compresses to tar gz
      CLOSED: [2020-07-16 Thu 11:32]
***** DONE Increase sequence lengths during training to accomodate for longer paraphrases, compute average seq lengths of wmt inputs to estimate model seq lengths for training paraphrase detector, work on keeping code simple
      CLOSED: [2020-07-14 Tue 14:53]
***** DONE consider making separate branch with sbatch parameters all present in files as necessary for reproducibility
      CLOSED: [2020-07-09 Thu 16:30]
***** DONE bug in XLM-R as it does not appear to learn -> look through code
      CLOSED: [2020-06-17 Wed 16:47]
***** DONE multilingual BERT with de only -> bug in how test scripts are saved leads to wrong results
      CLOSED: [2020-06-17 Wed 16:48]
***** DONE maybe consider using German BERT for doing this task explicitly for German, for our end task -> German BERT and RoBERTa for English to focus on exact task -> perhaps just use xtreme repo and keep only paws-x task -> clean up code and workflow for it -> error might be arising due to gradient clipping for very large model
      CLOSED: [2020-06-17 Wed 16:48]
***** DONE look into ParaBank2 and universal decompositional semantics -> not great paraphrases, no human curation
      CLOSED: [2020-06-05 Fri 14:28]
***** DONE look into Duolingo dataset for paraphrases -> no German target side
      CLOSED: [2020-06-05 Fri 13:56]
***** DONE add symbols for defaults in metavar default formatter, maybe add some other formatting tricks such as indents for defaults
      CLOSED: [2020-06-02 Tue 17:55]
***** DONE try installing java locally instead of root, if stanford parser is indeed necessary
      CLOSED: [2020-05-29 Fri 15:23]
***** DONE paraphrasing with SGCP -> very bad results on both original test and WMT data -> very sensitive to exemplar
      CLOSED: [2020-05-28 Thu 18:14]
***** DONE embed and cluser using universal sentence encoder (eg. BERT or LASER) -> use separate clusters for exemplar utility, make diverse collection and evaluate using metric or other NN
      CLOSED: [2020-05-28 Thu 17:52]
***** DONE find other sentence with maximum similarity and use that as exemplar, useparaphrase of best as exemplar, use pos-tags of sentence
      CLOSED: [2020-05-28 Thu 17:52]
***** DONE convert wmt datasets with derived exemplars into format pipe-able into SGCP -> needed before paraphrasing
      CLOSED: [2020-05-28 Thu 17:52]
***** DONE add workflow to download laser models with python -m laserembeddings download-models
      CLOSED: [2020-05-28 Thu 17:49]
***** DONE set up WMT 17 dev/test data and basic repo
      CLOSED: [2020-04-29 Wed 15:57]
***** DONE convert all processes to makefile for ease
      CLOSED: [2020-05-04 Mon 15:31]
***** DONE set up data downloading for all wmt sets with SacreBLEU
      CLOSED: [2020-05-17 Sun 21:58]

*** Brainstorming and logs
**** NMT training on S3IT GPUs
***** V100-16GB safest option for fp16 fast training, tested with 3584:16 and now testing out 7168:8
***** V100-32GB works great but many times slurms allocates it when it has ~100s MB left
***** K80 does not permit fp16 for faster training, goes into OOM when using with max-tokens 7168 and update-freq 8 -> although can be used for PAWS-X

**** LASER embeddings + dense layers
***** not very useful by itself, needs a larger token-touching model
***** models do not show generalization, ie. training loss decreases but development loss rises
***** need to access larger token-based models to leverage full power of NLP model

**** Semantic similarity metrics
***** multireference BLEU score, use multiple paraphrases and check for best BLEU score
***** perhaps modified BLEU, METEOR, CCG semantics lambda calculus
***** perhaps some combination of edit distance with wordnet metrics
***** or NN technique using sentence BERT and other encoders -> more quantitative and continuous, can apply Michel et al. 2019 techniques for robustness comparisons
***** semantic parsing to graph, role labelling, wordnet concepts connecting, framenet, frame semantic parsing, brown clusters, AMR parsing, IWCS workshop for discussions 

**** Paraphrase generation
***** Ideas for self-paraphrasing
****** consider logical model for paraphrases, active to passive syntaxes and other logical frameworks -> use dependency parse on manual examples and check for logical process to create meaningful permutations
****** permute-paraphrase using syntax-tree chunks and test paraphrses using a detect or LASER embeddings for agnosticism between source/target

***** Viable pre-developed dynamic paraphrase-generation frameworks
****** SOW-REAP [torch, python3, average-documented] -> generate paraphrases without exemplar sentence form, worth trying out -> still poor results and only SOW model appears to be robust
******* refactor/extract out SOW model, shorten pipeline in sow to reduce computation and make input simpler
******* make quick samples from SOW and hand-select good ones, test them manually on fairseq NMT system for en-de to probe robustness
******* fork sow repo and clean code, remove bugs and make better documented with dep tracking and clearer instructions
******* require nltk word tokenize before main processing

****** SGCP [torch, python3, well-documented] -> generate paraphrases given exemplar sentence form, limitation is that exemplar sentence is a hard dependency, poor performance and not very semantically sound paraphrases
******* ParaNMT is likely to be better than QQPos since latter was trained only on qns
******* BERT score, BERT, RoBERTa for detecting paraphrases and quality
******* hand-written exemplar for meaningful output
******* remove exemplar sentence and replace with syntax form
******* clustering is done by meaning and not syntax -> or try difference via standard parse -> or random
******* provision of syntax directly instead of exemplar sentence
******* fix bug in sgcp to write all outs on separate lines and to not compute any similarity
******* change k means to find best number of clusters
******* add various paraphrase generation styles for SGCP such as same cluster, other cluster and same as source
******* require nltk word tokenize before main processing
******* future-idea: end-to-end paraphrase generation with adversarial goal, but unrealistic given time-frame and support

***** Legacy frameworks
****** Pair-it [tensorflow, python3, poorly documented] -> has potential to work but requires major refactoring
****** SCPN [torch, python2.7, poorly documented] -> buggy, but some examples work

**** Data augmentation
***** look into nli adversarial datasets -> Nevin and Aatlantise
***** either look for paraphrase source and target pair which are closest to gold ones and augment data with these -> is safer to train with and can possibly improve overall translation quality
***** otherwise, find paraphrase which is close on source side but problematic on target side and augment these with gold target -> acts as a regularizing anchor and possibly adds some stability
***** Zipf's law should apply to syntax chunks, bias might still be present
***** anchor might still be useful, look for similar syntax on the target side that can be substituted -> maybe some kind of imitation to make augmented pairs 
***** consider contributing paraphrases to data augmentation libraries from research
***** noise is not problematic since there is already noise present in normal training data
***** meaning preserving + adversarial outcome -> then useful
***** augmentation is important if adversarial attack is successful, maybe syntax real-life frequency has effect
