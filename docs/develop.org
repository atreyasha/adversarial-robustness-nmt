*** To-do's

**** Paraphrase generation
***** TODO embed and cluser using universal sentence encoder (eg. BERT or LASER) -> use separate clusters for exemplar utility, make diverse collection and evaluate using metric or other NN
***** TODO find other sentence with maximum similarity and use that as exemplar, use paraphrase of best as exemplar, use pos-tags of sentence
***** TODO convert wmt datasets with derived exemplars into format pipe-able into SGCP -> needed before paraphrasing
***** TODO perhaps do paraphrasing also for WMT training data in order to get new datasets that could be used for future augmentation
***** try constructing exemplar sentences by hand to check if it works -> check if it works
***** QQPos is likely to be a better model
***** BERT score, BERT, RoBERTa for detecting paraphrases and quality
***** look into new libraries provided by Mathias -> how could this possibly help our research -> only seq2sick is provided
***** clustering is done by meaning and not syntax -> or try difference via standard parse -> or random
***** provision of syntax directly instead of exemplar sentence

***** Viable frameworks
****** SGCP [torch, python3, well-documented] -> generate paraphrases given exemplar sentence form, limitation is that exemplar sentence is a hard dependency
******* viable-idea: remove exemplar sentence and replace with syntax form
******* future-idea: end-to-end paraphrase generation with adversarial goal, but unrealistic given time-frame and support

***** Legacy frameworks
****** SOW-REAP [torch, python3, average-documented] -> generate paraphrases without exemplar sentence form, worth trying out
****** SCPN [torch, python2.7, poorly documented] -> buggy, but some examples work
****** Pair-it [tensorflow, python3, poorly documented] -> has potential to work but requires major refactoring

**** Code and documentation
***** TODO need to add build workflow for SGCP and also symlinking relevant data directly there -> possibly consider git-submodules -> need to add another workflow to ensure it is initialized properly
***** TODO clarify exact meaning of wmt dev set vs test set
***** TODO handle virtual environment in remote system better -> maybe with poetry or with venvs -> either way keep it clean and simple
***** TODO add wmt workflow to download training data as well
***** consider building readme and project using python -m framework
***** add documentation/acknowledgments to datasets and code, refactor major code used in SCPN to make it cleaner and better
***** add citations in readme as per general standard
***** add relevant gitignores

**** SOTA NMT models
***** download SOTA models from fairseq, start testing paraphrased samples on it and manually check out differences in results, see if this idea makes sense on a large scale
***** look for models that worked on WMT en-de datasets and work from there

**** Semantic similarity metrics
***** multireference BLEU score, use multiple paraphrases and check for best BLEU score
***** make table with all metrics, or use several language pairs to test this, pre-process data as per pre-trained model
***** think of useful semantic similarity metrics to make comparisons
***** perhaps modified BLEU, METEOR, CCG semantics lambda calculus
***** or NN technique using sentence BERT and other encoders -> more quantitative and continuous, can apply Michel et al. 2019 techniques for robustness comparisons
***** Semantic parsing to graph, wordnet concepts connecting, framenet, frame semantic parsing, brown clusters, AMR parsing, IWCS workshop for discussions

**** Downstream data augmenttion
***** dual approach -> either look for paraphrase source and target pair which are closest to gold ones and augment data with these -> is safer to train with and can possibly improve overall translation quality
***** otherwise, find paraphrase which is close on source side but problematic on target side and augment these with gold target -> acts as a regularizing anchor and possibly adds some stability -> need to check semantics
***** this would be future work, but presentation of work depends on how this is envisioned
***** Zipf's law should apply to syntax chunks, bias might still be present
***** anchor might still be useful, look for similar syntax on the target side that can be substituted -> maybe some kind of imitation to make augmented pairs 
***** consider contributing paraphrases to data augmentation libraries from research
***** augmentation might still be useful in any case, even if the anchor is different
***** noise is not problematic since there is already noise present in normal training data
***** meaning preserving + adversarial outcome -> then useful
***** augmentation is important if adversarial attack is successful, maybe syntax real-life frequency has effect

*** Completed
***** DONE set up WMT 17 dev/test data and basic repo
      CLOSED: [2020-04-29 Wed 15:57]
***** DONE convert all processes to makefile for ease
      CLOSED: [2020-05-04 Mon 15:31]
***** DONE add pipeline to download WMT 17 training data      
      CLOSED: [2020-05-04 Mon 15:37]
***** DONE set up data downloading for all wmt sets with SacreBLEU
      CLOSED: [2020-05-17 Sun 21:58]
